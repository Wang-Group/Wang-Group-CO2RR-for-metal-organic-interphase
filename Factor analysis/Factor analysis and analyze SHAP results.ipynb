{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#try to find interpretation of the regression results using all the features\n",
    "MFF_features = pd.read_csv('MFFmatrix-onlyS.csv')\n",
    "RDkit_features = pd.read_csv('Rdkitmatrix+sym descriptor-only S.csv')\n",
    "\n",
    "# Initialize the Normalizer\n",
    "scaler = StandardScaler()\n",
    "# Then, use the transform method to transform the original dataset\n",
    "scaled_MFF_features = scaler.fit_transform(MFF_features.values)\n",
    "scaled_MFF_features = pd.DataFrame(scaled_MFF_features,columns=MFF_features.columns)\n",
    "scaled_MFF_features.drop(columns=scaled_MFF_features.std()[scaled_MFF_features.std()==0].index,inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Then, use the transform method to transform the original dataset\n",
    "scaled_RDkit_features = scaler.fit_transform(RDkit_features.values)\n",
    "scaled_RDkit_features = pd.DataFrame(scaled_RDkit_features,columns=RDkit_features.columns)\n",
    "scaled_RDkit_features.drop(columns=scaled_RDkit_features.std()[scaled_RDkit_features.std()==0].index,inplace=True)\n",
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "RDkit_correlation = scaled_RDkit_features.corr()\n",
    "scaled_RDkit_features.to_csv('scaled_RDkit_features.csv')\n",
    "# # Check for perfect multicollinearity (correlation of 1 or -1)\n",
    "# perfect_multicollinearity = RDkit_correlation[(RDkit_correlation == 1) | (RDkit_correlation == -1)]\n",
    "# print(\"Perfect Multicollinearity (correlation of 1 or -1):\")\n",
    "# print(perfect_multicollinearity)\n",
    "\n",
    "# Remove one of the variables in each pair with perfect multicollinearity\n",
    "to_remove = set()\n",
    "for column in RDkit_correlation.columns:\n",
    "    if column not in to_remove:\n",
    "        # Get indices of columns with perfect multicollinearity with the current column\n",
    "        perfect_cols = RDkit_correlation.index[(RDkit_correlation[column] == 1) | (RDkit_correlation[column] == -1)].tolist()\n",
    "        perfect_cols.remove(column)  # Remove the column itself\n",
    "        # print(perfect_cols)\n",
    "        to_remove.update(perfect_cols)\n",
    "print(to_remove)\n",
    "column_to_keep = [column for column in scaled_RDkit_features.columns if column not in to_remove]\n",
    "scaled_RDkit_features = scaled_RDkit_features.loc[:, column_to_keep]   \n",
    "print(scaled_RDkit_features.shape)\n",
    "\n",
    "RDkit_correlation = scaled_RDkit_features.corr()\n",
    "\n",
    "# Convert the correlation matrix to a distance matrix\n",
    "# Distance is computed as 1 - correlation\n",
    "distance_matrix = 1 - RDkit_correlation.abs()\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "# We use 'average' linkage method but you can try 'single', 'complete', etc.\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method='average')\n",
    "\n",
    "# Get the order of the features based on hierarchical clustering\n",
    "dendro = dendrogram(linkage_matrix, no_plot=True)\n",
    "reordered_RDkit_features = [RDkit_correlation.columns[i] for i in dendro['leaves']]\n",
    "reordered_RDkit = scaled_RDkit_features[reordered_RDkit_features]\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "reordered_RDkit_corr = RDkit_correlation.loc[reordered_RDkit_features, reordered_RDkit_features]\n",
    "\n",
    "# Plot the reordered correlation matrix\n",
    "plt.figure(figsize=(60, 60))\n",
    "sns.heatmap(reordered_RDkit_corr, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix with Rdkitmatrix+sym descriptors Features Grouped by Hierarchical Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "MFF_correlation = scaled_MFF_features.corr()\n",
    "\n",
    "# Remove one of the variables in each pair with perfect multicollinearity\n",
    "to_remove = set()\n",
    "for column in MFF_correlation.columns:\n",
    "    if column not in to_remove:\n",
    "        # Get indices of columns with perfect multicollinearity with the current column\n",
    "        perfect_cols = MFF_correlation.index[(MFF_correlation[column] == 1) | (MFF_correlation[column] == -1)].tolist()\n",
    "        perfect_cols.remove(column)  # Remove the column itself\n",
    "        # print(perfect_cols)\n",
    "        to_remove.update(perfect_cols)\n",
    "print(to_remove)\n",
    "column_to_keep = [column for column in scaled_MFF_features.columns if column not in to_remove]\n",
    "scaled_MFF_features = scaled_MFF_features.loc[:, column_to_keep]   \n",
    "print(scaled_MFF_features.shape)\n",
    "scaled_MFF_features.to_csv('scaled_MFF_features.csv')\n",
    "MFF_correlation = scaled_MFF_features.corr()\n",
    "\n",
    "# Convert the correlation matrix to a distance matrix\n",
    "# Distance is computed as 1 - correlation\n",
    "distance_matrix = 1 - MFF_correlation.abs()\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "# We use 'average' linkage method but you can try 'single', 'complete', etc.\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method='average')\n",
    "\n",
    "# Get the order of the features based on hierarchical clustering\n",
    "dendro = dendrogram(linkage_matrix, no_plot=True)\n",
    "reordered_MFF_features = [MFF_correlation.columns[i] for i in dendro['leaves']]\n",
    "reordered_MFF = scaled_MFF_features[reordered_MFF_features]\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "reordered_MFF_corr = MFF_correlation.loc[reordered_MFF_features, reordered_MFF_features]\n",
    "\n",
    "# Plot the reordered correlation matrix\n",
    "plt.figure(figsize=(60, 60))\n",
    "sns.heatmap(reordered_MFF_corr, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix with MFF Features Grouped by Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:32:53.226439Z",
     "start_time": "2024-06-24T11:31:30.994217Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read two CSV files A and B\n",
    "A = pd.read_csv('MFFmatrix-onlyS.csv')\n",
    "B = pd.read_csv('Rdkitmatrix+sym descriptor-only S.csv')\n",
    "\n",
    "# Calculate the correlation coefficient matrix of A data\n",
    "A_correlation_matrix = A.corr()\n",
    "\n",
    "#Print the correlation coefficient matrix of A\n",
    "print(\"Correlation Matrix of A:\\n\", A_correlation_matrix)\n",
    "\n",
    "# Find the column names with a correlation coefficient of 1 (excluding autocorrelation)\n",
    "to_remove = set()\n",
    "for col in A_correlation_matrix.columns:\n",
    "    for idx in A_correlation_matrix.index:\n",
    "        if col != idx and A_correlation_matrix.loc[idx, col] == 1:\n",
    "            if col != idx:  # If the variable name is different\n",
    "                to_remove.add(col)\n",
    "                break  # Find one and break out of the loop\n",
    "\n",
    "# Print the name of the variable to be deleted\n",
    "print(\"Variables to remove:\\n\", to_remove)\n",
    "\n",
    "# Delete the columns with correlation coefficient 1\n",
    "A1 = A.drop(columns=to_remove)\n",
    "\n",
    "# Print updated A data\n",
    "print(\"Updated A Data:\\n\", A1)\n",
    "\n",
    "# Save the new A1 CSV file\n",
    "A1.to_csv('A1.csv', index=False)\n",
    "print(\"Updated A1 CSV saved to 'A1.csv'\")\n",
    "\n",
    "# Read A1 CSV file\n",
    "A1 = pd.read_csv('A1.csv')\n",
    "\n",
    "# Initialize the Normalizer\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardized A1 data\n",
    "A1_scaled = scaler.fit_transform(A1)\n",
    "A1_scaled = pd.DataFrame(A1_scaled, columns=A1.columns)\n",
    "A1_scaled.drop(columns=A1_scaled.std()[A1_scaled.std() == 0].index, inplace=True)\n",
    "\n",
    "# Standardized B data\n",
    "B_scaled = scaler.fit_transform(B)\n",
    "B_scaled = pd.DataFrame(B_scaled, columns=B.columns)\n",
    "B_scaled.drop(columns=B_scaled.std()[B_scaled.std() == 0].index, inplace=True)\n",
    "\n",
    "# Make sure there are no missing values ​​in the data\n",
    "A1.dropna(inplace=True)\n",
    "B_scaled.dropna(inplace=True)\n",
    "\n",
    "# Initialize an empty DataFrame to store the correlation coefficients\n",
    "correlation_matrix = pd.DataFrame(index=A1.columns, columns=B_scaled.columns)\n",
    "\n",
    "#Calculate the correlation coefficient\n",
    "for a_col in A1.columns:\n",
    "    for b_col in B_scaled.columns:\n",
    "        correlation = A1[a_col].corr(B_scaled[b_col])\n",
    "        correlation_matrix.at[a_col, b_col] = correlation\n",
    "\n",
    "# Convert the correlation matrix to floating point type\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "# Get the row name of the first column and the column name of the first row of a matrix\n",
    "first_column_row_names = correlation_matrix.index\n",
    "first_row_column_names = correlation_matrix.columns\n",
    "\n",
    "# Find duplicate column names\n",
    "columns_to_drop = [col_name for col_name in first_row_column_names if col_name in first_column_row_names]\n",
    "\n",
    "# Delete the columns with duplicate column names\n",
    "correlation_matrix = correlation_matrix.drop(columns=columns_to_drop)\n",
    "\n",
    "# Delete rows or columns where all the values ​​are NaN\n",
    "correlation_matrix.dropna(axis=0, how='all', inplace=True)\n",
    "correlation_matrix.dropna(axis=1, how='all', inplace=True)\n",
    "# Print the updated correlation matrix\n",
    "print(\"Updated Correlation Matrix:\\n\", correlation_matrix)\n",
    "print(correlation_matrix.shape)\n",
    "# Save the updated correlation matrix as a CSV file\n",
    "correlation_matrix.to_csv('updated_correlation_matrix_between_Original_and_Encoded_Features.csv')\n",
    "print(\"Updated correlation matrix saved to 'updated_correlation_matrix_between_Original_and_Encoded_Features.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:53:37.365740Z",
     "start_time": "2024-06-24T11:53:18.897489Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Reading CSV Files\n",
    "file_path = 'updated_correlation_matrix_between_Original_and_Encoded_Features.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "plt.figure(figsize=(300, 200))  # Resize image to fit smaller matrix\n",
    "ax=sns.heatmap(data, cmap='coolwarm', annot=False,center=0, annot_kws={\"size\": 100}, cbar=True)\n",
    "plt.title('Updated Correlation Matrix Heatmap between MFF and Physicochemical properties features', fontsize=40)\n",
    "plt.xlabel('Physicochemical properties', fontsize=100)\n",
    "plt.ylabel('MFF', fontsize=100)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=60) # Set the x-axis font size\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=80) # Set the y-axis font size\n",
    "# Set the color bar font size\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=110)  # Adjust the color bar tick font size\n",
    "#cbar.set_label('Correlation Value', fontsize=100)  # Set the color bar label and its font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T02:32:15.706162Z",
     "start_time": "2024-06-11T02:32:15.679526Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment\n",
    "\n",
    "# Read correlation matrix CSV file\n",
    "correlation_matrix_path = 'updated_correlation_matrix_between_Original_and_Encoded_Features.csv'\n",
    "updated_correlation_matrix = pd.read_csv(correlation_matrix_path, index_col=0)\n",
    "\n",
    "def correlate_features(feature_to_search):\n",
    "    threshold_similar = 0.7\n",
    "    threshold_explain = 0.4\n",
    "\n",
    "    similar_features = []\n",
    "    explain_features = []\n",
    "\n",
    "    if feature_to_search in reordered_RDkit.columns:\n",
    "        Similar = reordered_RDkit_corr.loc[feature_to_search, :]\n",
    "        for rdkit in reordered_RDkit.columns:\n",
    "            if rdkit != feature_to_search and abs(Similar[rdkit]) > threshold_similar:\n",
    "                similar_features.append((rdkit, Similar[rdkit]))\n",
    "        similar_features = sorted(similar_features, key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        # If no similar features are found, fill in 'None'\n",
    "        if not similar_features:\n",
    "            similar_features.append(('None', 'None'))\n",
    "\n",
    "        if feature_to_search in updated_correlation_matrix.index:\n",
    "            representation = updated_correlation_matrix.loc[feature_to_search, :]\n",
    "            for mff in updated_correlation_matrix.columns:\n",
    "                if pd.notna(representation[mff]) and abs(representation[mff]) > threshold_explain:\n",
    "                    explain_features.append((mff, representation[mff]))\n",
    "\n",
    "        if feature_to_search in updated_correlation_matrix.columns:\n",
    "            representation = updated_correlation_matrix.loc[:, feature_to_search]\n",
    "            for mff in updated_correlation_matrix.index:\n",
    "                if pd.notna(representation[mff]) and abs(representation[mff]) > threshold_explain:\n",
    "                    explain_features.append((mff, representation[mff]))\n",
    "\n",
    "        explain_features = list(set(explain_features))\n",
    "        explain_features = sorted(explain_features, key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        # If no explanatory features are found, fill in 'None'\n",
    "        if not explain_features:\n",
    "            explain_features.append(('None', 'None'))\n",
    "\n",
    "    elif feature_to_search in updated_correlation_matrix.index:\n",
    "        explain_features = ['MFF descriptor itself']\n",
    "\n",
    "    return similar_features, explain_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T02:36:28.327822Z",
     "start_time": "2024-06-11T02:36:28.218680Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the CSV file from which to read the feature_to_search list\n",
    "features_to_search_path = 'S-all metrics/shap_results-n-PrOH/MLP-top-features.csv'\n",
    "features_to_search_df = pd.read_csv(features_to_search_path, encoding='utf-8')\n",
    "# Path to the CSV file where you want to read product information\n",
    "product_info_path = 'S-all metrics/shap_results-n-PrOH/MLP-features-effects.csv'\n",
    "product_info_df = pd.read_csv(product_info_path, encoding='utf-8')\n",
    "\n",
    "# Debug information: Check column names\n",
    "print(\"Features to search columns:\", features_to_search_df.columns)\n",
    "print(\"Product info columns:\", product_info_df.columns)\n",
    "\n",
    "# Preprocessing to ensure that special characters do not affect subsequent operations\n",
    "features_to_search_df.columns = features_to_search_df.columns.str.replace('/', '_').str.replace(' ', '_')\n",
    "product_info_df.columns = product_info_df.columns.str.replace('/', '_').str.replace(' ', '_')\n",
    "\n",
    "# Debug information: Check replaced column names\n",
    "print(\"Processed features to search columns:\", features_to_search_df.columns)\n",
    "print(\"Processed product info columns:\", product_info_df.columns)\n",
    "\n",
    "# Create an empty list to store all the results\n",
    "results_list = []\n",
    "\n",
    "# Process each feature_to_search and fill the result list\n",
    "for index, row in features_to_search_df.iterrows():\n",
    "    feature_to_search = row[0]\n",
    "    similar_features, explain_features = correlate_features(feature_to_search)\n",
    "\n",
    "    product_info = product_info_df.loc[product_info_df['Feature'] == feature_to_search, 'Effect'].values\n",
    "    product_value = product_info[0] if len(product_info) > 0 else ''\n",
    "\n",
    "    # Ensure that all entries are tuples containing two values\n",
    "    similar_features = [item if isinstance(item, tuple) else ('None', 'None') for item in similar_features]\n",
    "    \n",
    "    max_len = max(len(similar_features), len(explain_features), 1)\n",
    "#Change the name here\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Descriptor of interest': [feature_to_search] * max_len,\n",
    "        'Effect on n-PrOH': [product_value] * max_len,\n",
    "        'Similar Features': [f\"{feature}, {correlation}\" for feature, correlation in similar_features] + [''] * (max_len - len(similar_features)),\n",
    "        'MFF': [f\"{item[0]}, {item[1]}\" if isinstance(item, tuple) else item for item in explain_features] + [''] * (max_len - len(explain_features))\n",
    "    })\n",
    "\n",
    "    results_list.append(temp_df)\n",
    "\n",
    "results_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "csv_save_dir = '20240611-S-data explanation'\n",
    "os.makedirs(csv_save_dir, exist_ok=True)\n",
    "\n",
    "csv_file_path = os.path.join(csv_save_dir, 'n-PrOH-features_analysis_results.csv')\n",
    "results_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "excel_file_path = os.path.join(csv_save_dir, 'n-PrOH-features_analysis_results.xlsx')\n",
    "results_df.to_excel(excel_file_path, index=False, sheet_name='Analysis Results')\n",
    "\n",
    "wb = load_workbook(excel_file_path)\n",
    "ws = wb['Analysis Results']\n",
    "\n",
    "current_feature = None\n",
    "start_row = 2\n",
    "\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    feature = ws[f'A{row}'].value\n",
    "    if feature == current_feature:\n",
    "        continue\n",
    "    else:\n",
    "        if current_feature is not None:\n",
    "            ws.merge_cells(start_row=start_row, start_column=1, end_row=row-1, end_column=1)\n",
    "            ws[f'A{start_row}'].alignment = Alignment(vertical='center')\n",
    "        current_feature = feature\n",
    "        start_row = row\n",
    "\n",
    "if current_feature is not None:\n",
    "    ws.merge_cells(start_row=start_row, start_column=1, end_row=ws.max_row, end_column=1)\n",
    "    ws[f'A{start_row}'].alignment = Alignment(vertical='center')\n",
    "\n",
    "current_feature = None\n",
    "current_effect = None\n",
    "start_row = 2\n",
    "\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    effect = ws[f'B{row}'].value\n",
    "    if effect == current_effect:\n",
    "        continue\n",
    "    else:\n",
    "        if current_effect is not None:\n",
    "            ws.merge_cells(start_row=start_row, start_column=2, end_row=row-1, end_column=2)\n",
    "            ws[f'B{start_row}'].alignment = Alignment(vertical='center')\n",
    "        current_effect = effect\n",
    "        start_row = row\n",
    "\n",
    "if current_effect is not None:\n",
    "    ws.merge_cells(start_row=start_row, start_column=2, end_row=ws.max_row, end_column=2)\n",
    "    ws[f'B{start_row}'].alignment = Alignment(vertical='center')\n",
    "\n",
    "wb.save(excel_file_path)\n",
    "print(f\"Results saved to '{csv_file_path}' and '{excel_file_path}'\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:06:23.766937Z",
     "start_time": "2024-06-22T05:05:46.046624Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer, calculate_kmo, calculate_bartlett_sphericity\n",
    "\n",
    "# Read the correlation coefficient matrix CSV file\n",
    "file_path = 'updated_correlation_matrix_between_Original_and_Encoded_Features.csv'\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Preprocess the data to remove rows containing NaN or infinite values\n",
    "df = df.loc[~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
    "\n",
    "# Transpose the data so that each row is a variable\n",
    "df_transposed = df.transpose()\n",
    "\n",
    "# Check the suitability of the data\n",
    "kmo_all, kmo_model = calculate_kmo(df_transposed)\n",
    "bartlett_chi_square, bartlett_p_value = calculate_bartlett_sphericity(df_transposed)\n",
    "\n",
    "print(f\"KMO Test: {kmo_model}\")\n",
    "print(f\"Bartlett's Test: Chi-square={bartlett_chi_square}, p-value={bartlett_p_value}\")\n",
    "\n",
    "if bartlett_p_value < 0.05:\n",
    "    print(\"The data is suitable for factor analysis.\")\n",
    "    \n",
    "    # Factor Analysis\n",
    "    Load_Matrix = FactorAnalyzer(n_factors=len(df_transposed.T), rotation=None, method='principal')\n",
    "    Load_Matrix.fit(df_transposed)\n",
    "    \n",
    "    # Draw a scree plot to determine the number of factors\n",
    "    ev, v = Load_Matrix.get_eigenvalues()\n",
    "    print('\\n Correlation matrix eigenvalues：', ev)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(range(1, df_transposed.shape[1] + 1), ev)\n",
    "    plt.plot(range(1, df_transposed.shape[1] + 1), ev)\n",
    "    plt.title('Changes in eigenvalues ​​and number of factors', fontdict={'weight': 'normal', 'size': 25})\n",
    "    plt.xlabel('Factors', fontdict={'weight': 'normal', 'size': 15})\n",
    "    plt.ylabel('Eigenvalues', fontdict={'weight': 'normal', 'size': 15})\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Factor analysis after rotation\n",
    "    Load_Matrix_rotated = FactorAnalyzer(rotation='varimax', n_factors=18, method='principal')\n",
    "    Load_Matrix_rotated.fit(df_transposed)\n",
    "    f_contribution_var_rotated = Load_Matrix_rotated.get_factor_variance()\n",
    "    matrices_var_rotated = pd.DataFrame()\n",
    "    matrices_var_rotated[\"Eigenvalue\"] = f_contribution_var_rotated[0]\n",
    "    matrices_var_rotated[\"Variance Contribution\"] = f_contribution_var_rotated[1]\n",
    "    matrices_var_rotated[\"Cumulative contribution of variance\"] = f_contribution_var_rotated[2]\n",
    "    print(\"Contribution rate of the rotated load matrix\")\n",
    "    print(matrices_var_rotated)\n",
    "    print(\"Rotated component matrix\")\n",
    "    print(Load_Matrix_rotated.loadings_)\n",
    "    \n",
    "    # Visualizing the factor loading matrix\n",
    "    Load_Matrix = Load_Matrix_rotated.loadings_\n",
    "    df_loadings = pd.DataFrame(np.abs(Load_Matrix), index=df_transposed.columns, columns=[f'Factor{i+1}' for i in range(18)])\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Times New Roman' # Set the English font toTimes New Roman \n",
    "    plt.figure(figsize=(100, 100))\n",
    "    ax = sns.heatmap(df_loadings, center=0, annot=True, cmap=\"coolwarm\", cbar=False, fmt=\".2f\", annot_kws={\"size\": 30})\n",
    "    ax.xaxis.set_tick_params(labelsize=40) # Set the x-axis font size\n",
    "    ax.yaxis.set_tick_params(labelsize=40) # Set the y-axis font size\n",
    "    plt.title('Factor Loadings Matrix', fontsize=40)\n",
    "    plt.ylabel('Variables', fontsize=40)# Set the y-axis label\n",
    "    plt.xlabel('Factors', fontsize=40)\n",
    "    plt.show()# Show image\n",
    "    \n",
    "    # Calculating factor scores\n",
    "    factor_scores = Load_Matrix_rotated.transform(df_transposed)\n",
    "    df_scores = pd.DataFrame(factor_scores, index=df_transposed.index, columns=[f'Factor{i+1}' for i in range(18)])\n",
    "    \n",
    "    # Printing and saving factor scores\n",
    "    print(\"Factor Scores:\")\n",
    "    print(df_scores)\n",
    "    \n",
    "    scores_file_path = 'factor_scores.csv'\n",
    "    df_scores.to_csv(scores_file_path)\n",
    "    print(f\"Factor scores saved to {scores_file_path}\")\n",
    "    # Heatmap to visualize factor scores\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'  # Set the English font toTimes New Roman \n",
    "    plt.figure(figsize=(100, 200))\n",
    "    ax = sns.heatmap(df_scores, annot=True, center=0, cmap=\"coolwarm\", cbar=False, fmt=\".2f\", annot_kws={\"size\": 30})\n",
    "    plt.title('Factor Scores of molecular physicochemical properties', fontsize=40)\n",
    "    plt.xlabel('Factors', fontsize=40)\n",
    "    plt.ylabel('Physicochemical properties', fontsize=40)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=40) # Set the x-axis font size\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=40) # Set the y-axis font size\n",
    "    plt.show()  # Show image\n",
    "else:\n",
    "    print(\"The data is not suitable for factor analysis.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "760.148px",
    "left": "1813.65px",
    "right": "20px",
    "top": "115.992px",
    "width": "758.625px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
